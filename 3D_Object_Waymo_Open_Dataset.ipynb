{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RedwanNewaz/3DPedestrainDetection/blob/master/3D_Object_Waymo_Open_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pVhOfzLx9us"
      },
      "source": [
        "#Waymo Open Dataset Tutorial\n",
        "\n",
        "- Website: https://waymo.com/open\n",
        "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
        "\n",
        "This tutorial demonstrates how to use the Waymo Open Dataset with two frames of data. Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
        "\n",
        "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
        "\n",
        "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWdJUWiZdMte"
      },
      "source": [
        "## Install waymo_open_dataset package"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf waymo-od > /dev/null\n",
        "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
        "!cd waymo-od && git branch -a\n",
        "!cd waymo-od && git checkout remotes/origin/r1.0\n",
        "!pip3 install --upgrade pip"
      ],
      "metadata": {
        "id": "S8c5IbZeZImD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDQ1DPqwdfNW"
      },
      "source": [
        "!pip3 install waymo-open-dataset-tf-2-6-0\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "from waymo_open_dataset.utils import range_image_utils\n",
        "from waymo_open_dataset.utils import transform_utils\n",
        "from waymo_open_dataset.utils import  frame_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibor0U9XBlX6"
      },
      "source": [
        "## Read one frame\n",
        "\n",
        "Each file in the dataset is a sequence of frames ordered by frame start timestamps. We have extracted two frames from the dataset to demonstrate the dataset format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FusedSensors:\n",
        "    def __init__(self, frame):\n",
        "        self.frame = frame\n",
        "        (self.range_images, self.camera_projections, self.seg_labels,\n",
        "         self.range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(\n",
        "            frame)\n",
        "\n",
        "    def extract_lidar_data(self):\n",
        "        points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
        "            self.frame,\n",
        "            self.range_images,\n",
        "            self.camera_projections,\n",
        "            self.range_image_top_pose)\n",
        "        # 3d points in vehicle frame.\n",
        "        points_all = np.concatenate(points, axis=0)\n",
        "        cp_points_all = np.concatenate(cp_points, axis=0)\n",
        "        return points_all, cp_points_all\n",
        "\n",
        "    def extract_ri2_data(self):\n",
        "        points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
        "            self.frame,\n",
        "            self.range_images,\n",
        "            self.camera_projections,\n",
        "            self.range_image_top_pose,\n",
        "            ri_index=1)\n",
        "\n",
        "        points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
        "        cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)\n",
        "        return points_all_ri2, cp_points_all_ri2\n",
        "\n",
        "    def __extract_images(self):\n",
        "        images = sorted(self.frame.images, key=lambda i: i.name)\n",
        "        return images\n",
        "\n",
        "    def get_front_image(self):\n",
        "        front_cam_index = 0\n",
        "        raw_img = self.__extract_images()[front_cam_index].image\n",
        "        return tf.image.decode_jpeg(raw_img).numpy()\n",
        "\n",
        "    def projected_points(self):\n",
        "        points_all, cp_points_all = self.extract_lidar_data()\n",
        "\n",
        "        # The distance between lidar points and vehicle frame origin.\n",
        "        points_all_tensor = tf.norm(points_all, axis=-1, keepdims=True)\n",
        "        cp_points_all_tensor = tf.constant(cp_points_all, dtype=tf.int32)\n",
        "        images = self.__extract_images()\n",
        "        mask = tf.equal(cp_points_all_tensor[..., 0], images[0].name)\n",
        "\n",
        "        cp_points_all_tensor = tf.cast(tf.gather_nd(\n",
        "            cp_points_all_tensor, tf.where(mask)), dtype=tf.float32)\n",
        "        points_all_tensor = tf.gather_nd(points_all_tensor, tf.where(mask))\n",
        "\n",
        "        projected_points_all_from_raw_data = tf.concat(\n",
        "            [cp_points_all_tensor[..., 1:3], points_all_tensor], axis=-1).numpy()\n",
        "        return projected_points_all_from_raw_data"
      ],
      "metadata": {
        "id": "-62xj-eqU49f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29uZtYLJBx2r"
      },
      "source": [
        "# tf.enable_eager_execution()\n",
        "FILENAME = '/content/waymo-od/tutorial/frames'\n",
        "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
        "for data in dataset:\n",
        "    frame = open_dataset.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    data = FusedSensors(frame)\n",
        "    pcl, cp_pcl = data.extract_lidar_data()\n",
        "    image = data.get_front_image()\n",
        "    projected_pcl = data.projected_points()\n",
        "    print(pcl.shape)\n",
        "    \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute maximum and minimum ranges\n"
      ],
      "metadata": {
        "id": "M_FnBfavIVbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('projected pcl max depth = ', np.max(projected_pcl.T[2, :] ), \" min depth = \", np.min(projected_pcl.T[2, :] ))\n",
        "print('raw pcl max depth = ', np.max(pcl.T[0, :] ), \" min depth = \", np.min(pcl.T[0, :] ))"
      ],
      "metadata": {
        "id": "eEO92VQWIceK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object Detection"
      ],
      "metadata": {
        "id": "adiiz2zhMLd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check nvcc version\n",
        "!nvcc -V\n",
        "# Check GCC version\n",
        "!gcc --version"
      ],
      "metadata": {
        "id": "2lJXxhXoMNdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies: (use cu111 because colab has CUDA 11.1)\n",
        "!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install mmcv-full thus we could use CUDA operators\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
        "\n",
        "# Install mmdetection\n",
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "1GBSN3W4MU80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We download the pre-trained checkpoints for inference and finetuning.\n",
        "!mkdir checkpoints\n",
        "!wget -c https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco_20210526_095054-1f77628b.pth \\\n",
        "      -O checkpoints/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco_20210526_095054-1f77628b.pth"
      ],
      "metadata": {
        "id": "IfQMwTYFMfI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mmcv\n",
        "from mmcv.runner import load_checkpoint\n",
        "\n",
        "from mmdet.apis import inference_detector, show_result_pyplot\n",
        "from mmdet.models import build_detector\n",
        "\n",
        "# Choose to use a config and initialize the detector\n",
        "config = 'configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco.py'\n",
        "# Setup a checkpoint file to load\n",
        "checkpoint = 'checkpoints/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco_20210526_095054-1f77628b.pth'\n",
        "\n",
        "# Set the device to be used for evaluation\n",
        "device='cuda:0'\n",
        "device=False\n",
        "\n",
        "# Load the config\n",
        "config = mmcv.Config.fromfile(config)\n",
        "# Set pretrained to be None since we do not need pretrained model here\n",
        "config.model.pretrained = None\n",
        "\n",
        "# Initialize the detector\n",
        "model = build_detector(config.model)\n",
        "\n",
        "# Load checkpoint\n",
        "# checkpoint = load_checkpoint(model, checkpoint, map_location=device)\n",
        "checkpoint = load_checkpoint(model, checkpoint)\n",
        "\n",
        "# Set the classes of models for inference\n",
        "model.CLASSES = checkpoint['meta']['CLASSES']\n",
        "\n",
        "# We need to set the model's cfg for inference\n",
        "model.cfg = config\n",
        "\n",
        "# Convert the model to GPU\n",
        "# model.to(device)\n",
        "# Convert the model into evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "HulWp3syMwfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert projected point cloud to a dictionary"
      ],
      "metadata": {
        "id": "jl4oeE44a-TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "projected_pcl_dict = {}\n",
        "for point in projected_pcl:\n",
        "  projected_pcl_dict[(point[0], point[1])] = point[2]"
      ],
      "metadata": {
        "id": "jYyB_oFaa8uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib.patches import Polygon\n",
        "import scipy.stats as st\n",
        "\n",
        "img = image.copy()\n",
        "# plt.imshow(img)\n",
        "# ax = plt.gca()\n",
        "H, W, _ = img.shape\n",
        "result = inference_detector(model, img)\n",
        "for i, temp in enumerate(np.squeeze(result)):\n",
        "  for bbox in temp:\n",
        "    conf = bbox[-1]\n",
        "    if conf < 0.5: continue \n",
        "    bbox_int = bbox.astype(np.int32)\n",
        "    patch = img[bbox_int[1]:bbox_int[3], bbox_int[0]:bbox_int[2], :]\n",
        "\n",
        "    depths = []\n",
        "    for y in range(bbox_int[1], bbox_int[3]):\n",
        "      for x in range(bbox_int[0], bbox_int[2]):\n",
        "        d = projected_pcl_dict.get((x, y))\n",
        "        if d is not None:\n",
        "          depths.append(d)\n",
        "    \n",
        "    if(depths):\n",
        "      # print(np.mean(depths), np.std(depths))\n",
        "      X = int(np.mean(depths))\n",
        "      std = np.sqrt(np.std(depths))\n",
        "      z = (X - np.mean(depths)) /  std\n",
        "      print(f\"dist = {X} conf = {1.0 - st.norm.cdf(z):.4f} std = {std : .4f}, samples = {len(depths)}\")\n",
        "\n",
        "    \n",
        "      \n",
        "\n",
        "\n",
        "    plt.imshow(patch)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # poly = [[bbox_int[0], bbox_int[1]], [bbox_int[0], bbox_int[3]],\n",
        "    #         [bbox_int[2], bbox_int[3]], [bbox_int[2], bbox_int[1]]]\n",
        "    # np_poly = np.array(poly).reshape((4, 2)) \n",
        "    # ax.add_patch(Polygon(np_poly))\n",
        "    # print(poly)\n",
        "    # print(patch.shape, conf)\n",
        "    \n",
        "\n",
        "show_result_pyplot(model, img, result, score_thr=0.3)"
      ],
      "metadata": {
        "id": "iX1Z2rOMM7xJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}